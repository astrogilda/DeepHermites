### Hermite Networks generate smoother landscape than ReLU networks. 

Code for Figure 7 of https://arxiv.org/abs/1909.05479.pdf

(directory: lossvalues) Magnitude of loss and its variation is lower for Hermites.
(directory: betasmooth) Gradients are more stable on Hermite loss landscape as indicated by lower maximum beta smoothness. 
(directory: deltaweight)  Hermite networks have a higher effective learning rate .
(directory: sparsity): Hermite nets have higher active units than ReLU nets.
